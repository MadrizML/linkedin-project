{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "import unidecode\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_unidecode(x):\n",
    "    return unidecode.unidecode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove ponctuation\n",
    "def remove_punct1(x):\n",
    "    p = ['!', '(', ')', '-', '[', ']', '{', '}', ';', ':', \"'\", '\"', '\\\\', ',', '<', '>', '.', '/', '?', '@', '#', '$', '%', '^', '&', '*', '_', '~']\n",
    "    return ' '.join([string for string in x.split(' ') if string not in p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove ponctuation\n",
    "def remove_punct2(x):\n",
    "    import re\n",
    "    return re.sub(r'[^\\w]|[^\\w] ', ' ', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translation(x):\n",
    "    if detect(x) != 'en':\n",
    "        x = translator.translate(x).text\n",
    "        return x\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stop words to clean text (job description)\n",
    "stopwords_en = set(stopwords.words('english'))\n",
    "\n",
    "stopwords_pt = set(stopwords.words('portuguese'))\n",
    "\n",
    "stopwords_es = set(stopwords.words('spanish'))\n",
    "\n",
    "stopwords_de = set(stopwords.words('german'))\n",
    "\n",
    "stopwords_nl = set(stopwords.words('dutch'))\n",
    "\n",
    "stopwords_fr = set(stopwords.words('french'))\n",
    "\n",
    "stopwords_fi = set(stopwords.words('finnish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(x):\n",
    "    return ' '.join([word.lower() for word in x.split() \\\n",
    "                   if word.lower() not in (stopwords_en)]) if detect(x)=='en'\\\n",
    "    else ' '.join([word.lower() for word in x.split() \\\n",
    "                   if word.lower() not in (stopwords_pt)]) if detect(x)=='pt'\\\n",
    "    else ' '.join([word.lower() for word in x.split() \\\n",
    "                   if word.lower() not in (stopwords_es)]) if detect(x)=='es'\\\n",
    "    else ' '.join([word.lower() for word in x.split() \\\n",
    "                   if word.lower() not in (stopwords_de)]) if detect(x)=='de'\\\n",
    "    else ' '.join([word.lower() for word in x.split() \\\n",
    "                   if word.lower() not in (stopwords_fr)]) if detect(x)=='fr'\\\n",
    "    else ' '.join([word.lower() for word in x.split() \\\n",
    "                   if word.lower() not in (stopwords_nl)]) if detect(x)=='nl'\\\n",
    "    else ' '.join([word.lower() for word in x.split() \\\n",
    "                   if word.lower() not in (stopwords_fi)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words1(x):\n",
    "    stop_words_concat = list(stopwords_en)+list(stopwords_es)+list(stopwords_pt)\n",
    "    return ' '.join([word.lower() for word in x.split() \\\n",
    "                   if word.lower() not in stop_words_concat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def stem_snowball(x):\n",
    "    if detect(x) == 'pt':\n",
    "        stemmer = SnowballStemmer('portuguese')\n",
    "        tokens = nltk.word_tokenize(x)\n",
    "        return [stemmer.stem(token) for token in tokens]\n",
    "    elif detect(x) == 'en':\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        tokens = nltk.word_tokenize(x)\n",
    "        return [stemmer.stem(token) for token in tokens]\n",
    "    elif detect(x) == 'es':\n",
    "        stemmer = SnowballStemmer('spanish')\n",
    "        tokens = nltk.word_tokenize(x)\n",
    "        return [stemmer.stem(token) for token in tokens]\n",
    "    elif detect(x) == 'de':\n",
    "        stemmer = SnowballStemmer('german')\n",
    "        tokens = nltk.word_tokenize(x)\n",
    "        return [stemmer.stem(token) for token in tokens]\n",
    "    elif detect(x) == 'nl':\n",
    "        stemmer = SnowballStemmer('dutch')\n",
    "        tokens = nltk.word_tokenize(x)\n",
    "        return [stemmer.stem(token) for token in tokens]\n",
    "    elif detect(x) == 'fr':\n",
    "        stemmer = SnowballStemmer('french')\n",
    "        tokens = nltk.word_tokenize(x)\n",
    "        return [stemmer.stem(token) for token in tokens]\n",
    "    elif detect(x) == 'fi':\n",
    "        stemmer = SnowballStemmer('finnish')\n",
    "        tokens = nltk.word_tokenize(x)\n",
    "        return [stemmer.stem(token) for token in tokens]\n",
    "    else:\n",
    "        return nltk.word_tokenize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams(x):\n",
    "    try:\n",
    "        #Tokens = nltk.word_tokenize(x)\n",
    "        output = list(nltk.bigrams(x))\n",
    "        return output\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram(x):\n",
    "    try:\n",
    "        #Tokens = nltk.word_tokenize(x)\n",
    "        output = list(nltk.trigrams(x))\n",
    "        return output\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(x):\n",
    "    return nltk.word_tokenize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2384, 12)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/home/inrx/Ironhack/TA/job-search/linkedin-project/linkedin_search.csv')\n",
    "print (df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1801, 12)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates(keep='last')\n",
    "df = df.reset_index(drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lang'] = df['role-description'].apply(lambda x: detect(x))\n",
    "\n",
    "df = df[df['lang'].map(df['lang'].value_counts()) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(to_replace=r'\\n', value=' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['role-description'] = df['role-description'].apply(remove_punct1)\n",
    "df['role-description'] = df['role-description'].apply(remove_punct2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['role-description'] = df['role-description'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['role_description1'] = df['role-description'].apply(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.role_description1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['role_description1'] = df['role_description1'].apply(convert_unidecode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text according remove the stopwords according to the language of the job post\n",
    "df['role_description1'] = df['role_description1'].apply(remove_stop_words)\n",
    "\n",
    "# Clean the text according remove the stopwords regardless the language of the job post\n",
    "df['role_description1'] = df['role_description1'].apply(remove_stop_words1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stem = df[['link', 'role_description1']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stem['role_description1'] = df_stem['role_description1'].apply(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_stem['role_des_bigrams'] = df_stem['role_description1'].apply(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stem['role_des_trigrams'] = df_stem['role_description1'].apply(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_stem.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store df_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams_list = [trigram for lst in df_stem.role_des_trigrams for trigram in lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams_list = [tuple(sorted (trigram)) for trigram in trigrams_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = {tup:trigrams_list.count(tup) for tup in trigrams_list}\n",
    "\n",
    "#trigrams = {k: v for k, v in trigrams.items() if v > 2}\n",
    "\n",
    "trigrams_sorted = sorted(trigrams.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store trigrams_list\n",
    "%store trigrams \n",
    "%store trigrams_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_list = [bigram for lst in df_stem.role_des_bigrams for bigram in lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = {tup:bigrams_list.count(tup) for tup in bigrams_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigrams = {k: v for k, v in bigrams.items() if v > 2}\n",
    "\n",
    "bigrams_sorted = sorted(bigrams.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store bigrams_list\n",
    "%store bigrams\n",
    "%store bigrams_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokens_list = [word for lst in df_stem.role_description1 for word in lst]\n",
    "\n",
    "tokens = {word:tokens_list.count(word) for word in tokens_list}\n",
    "\n",
    "tokens_sorted = sorted(tokens.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = {word:tokens_list.count(word) for word in tokens_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_sorted = sorted(tokens.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store tokens_list\n",
    "%store tokens\n",
    "%store tokens_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [word.strip() for word in df['function'] if word !='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_dict = {word:functions.count(word) for word in functions}\n",
    "functions_dict = sorted(functions_dict.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def separate_capitalized(lst):\n",
    "    s_list = re.findall('[A-Z][^A-Z]*', lst)\n",
    "    ss_list = [nlst.split(' ') for nlst in s_list]\n",
    "    return (' '.join([word.title() for lst in ss_list for word in lst if word != '']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_capitalized(functions_dict)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for lst in functions_dict:\n",
    "    print (translation(lst[0]))\n",
    "    #functions_dict[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['function'][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
