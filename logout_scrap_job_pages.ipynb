{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_job_pages():\n",
    "    # import libraries\n",
    "    from selenium import webdriver\n",
    "    from parsel import Selector\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    import time\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import csv\n",
    "    import random\n",
    "    import re\n",
    "    from tqdm import tqdm\n",
    "    from tqdm import tqdm_gui\n",
    "\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "    #driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    path = 'job_links/data_analyst_European_Economic_Area_links.csv'\n",
    "\n",
    "    with open(path, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        your_list = list(reader)\n",
    "\n",
    "    page_list_raw = your_list[0]\n",
    "    page_list = page_list_raw[156:]\n",
    "\n",
    "    # Create for loop to scrap data from job posting pages\n",
    "    number = 0\n",
    "\n",
    "    for page in tqdm(page_list, desc='Scraping job details'):\n",
    "        job_path = page\n",
    "        driver.get(job_path)\n",
    "\n",
    "        # sleep to avoid crawler detection\n",
    "        time.sleep(round(random.uniform(1,1.5), 2))\n",
    "\n",
    "        # scrap job title\n",
    "        job_title = driver.find_element_by_class_name('topcard__title')\n",
    "        job_title_final = job_title.text\n",
    "        # topcard__title\n",
    "\n",
    "        # scrap company name \n",
    "        try:\n",
    "            company_raw = driver.find_element_by_class_name('topcard__org-name-link')\n",
    "            company_final = company_raw.text\n",
    "        except: #topcard__flavor\n",
    "            company_raw = driver.find_element_by_class_name('topcard__flavor')\n",
    "            company_final = company_raw.text\n",
    "        #location_final = ''\n",
    "\n",
    "        # scrap company link\n",
    "        try:\n",
    "            company_link_raw = driver.find_element_by_class_name(\"topcard__org-name-link\")\n",
    "            company_link_final = company_link_raw.get_attribute('href')\n",
    "        except:\n",
    "            company_link_final = np.NaN\n",
    "\n",
    "\n",
    "        # scrap number of applicants and post date\n",
    "        applications_raw = driver.find_element_by_class_name(\"num-applicants__caption\")\n",
    "        applications_final = applications_raw.text\n",
    "        if applications_final == 'Seja um dos 25 primeiros candidatos':\n",
    "            applications_final = '<25'\n",
    "\n",
    "\n",
    "        ### obtain post date and number of applicants\n",
    "        post_date_raw = driver.find_element_by_class_name('topcard__flavor--metadata')\n",
    "        post_date = post_date_raw.text\n",
    "\n",
    "        # Srap the full description of the job\n",
    "        role = driver.find_element_by_class_name(\"description__text\")\n",
    "        role_final = role.text\n",
    "\n",
    "\n",
    "        ## Scrap Location\n",
    "        location_raw = driver.find_element_by_xpath(\"//span[@class='topcard__flavor topcard__flavor--bullet']\")\n",
    "        location_final = location_raw.text\n",
    "\n",
    "\n",
    "        ## Scrap job critera\n",
    "        criteria_raw = driver.find_elements_by_class_name('job-criteria__list')\n",
    "        criteria = [x.text for x in criteria_raw]\n",
    "        criteria_1 = [x.split('\\n') for x in criteria]\n",
    "        ### Scrap seniority level\n",
    "        experience = criteria_1[0][1]\n",
    "        ### Scrap type of contract\n",
    "        type_work = criteria_1[0][3]\n",
    "        ### Scrap the function type\n",
    "        function = criteria_1[0][5]\n",
    "        ### Scrap the sector of the company\n",
    "        sector = criteria_1[0][7]\n",
    "        sector_final = re.compile(\"(?=[A-Z])\").split(sector)\n",
    "\n",
    "\n",
    "        # Create a row with all the scrapped info to append to the .csv file\n",
    "        row = [job_title_final, \n",
    "               company_final, \n",
    "               company_link_final, \n",
    "               post_date, \n",
    "               applications_final, \n",
    "               location_final, \n",
    "               role_final, \n",
    "               job_path,\n",
    "               experience,\n",
    "               type_work,\n",
    "               function,\n",
    "               sector_final]\n",
    "\n",
    "\n",
    "        # Append each row in the end of the existing .csv file\n",
    "        with open('linkedin_search.csv','a',newline='') as f:\n",
    "            writer=csv.writer(f)\n",
    "            writer.writerow(row)\n",
    "            number += 1\n",
    "\n",
    "    driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
